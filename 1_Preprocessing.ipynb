{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import OpenLA as la\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_txt_path = r\".\\data\\actions_txt\"\n",
    "Edudata = r'.\\data\\EduData_20221028'\n",
    "courses = [\"A-2020\",\"D-2020\",\"A-2021\",\"D-2021\",\"A-2022\",\"D-2022\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Operation_dict = {\n",
    "        'NEXT':'N',\n",
    "        'PREV':'P',\n",
    "        'ADD MARKER':'A',\n",
    "\n",
    "        'OPEN':'O',             \n",
    "        'CLOSE':'C',            \n",
    "        'PAGE_JUMP':'J',        \n",
    "        'GETIT':'G',            \n",
    "\n",
    "        'DELETE MARKER':'E',\n",
    "        'BOOKMARK_JUMP':'E',\n",
    "        'ADD BOOKMARK':'E',\n",
    "        'NOTGETIT':'E',\n",
    "        'ADD MEMO':'E',\n",
    "        'MEMO_TEXT_CHANGE_HISTORY':'E',\n",
    "\n",
    "        'DELETE BOOKMARK':'E',\n",
    "        'CHANGE MEMO':'E',\n",
    "        'SEARCH_JUMP':'E',\n",
    "        'REGIST CONTENTS':'E',\n",
    "        'DELETE_MEMO':'E',\n",
    "        'SEARCH':'E',\n",
    "        'OPEN_RECOMMENDATION':'E',\n",
    "        'CLICK_RECOMMENDATION':'E',\n",
    "        'TIMER_PAUSE':'E',\n",
    "        'TIMER_STOP':'E',\n",
    "        'ADD_HW_MEMO':'E',\n",
    "\n",
    "        'CLOSE_RECOMMENDATION':'E',\n",
    "        'CLEAR_HW_MEMO':'E',\n",
    "        'LINK_CLICK':'E',\n",
    "        'UNDO_HW_MEMO':'E',\n",
    "        'ADD_RECOMMENDATION' : 'E',\n",
    "        'REDO_HW_MEMO' : 'E',\n",
    "        'DELETE_RECOMMENDATION' : 'E',\n",
    "        'MEMO_JUMP' : 'E'\n",
    "        }\n",
    "\n",
    "\n",
    "def get_learninglog_sentences(eventstream, userid):\n",
    "    user_eventstream_df = get_user_eventstream(eventstream,userid)\n",
    "    return get_oneuser_sentences(user_eventstream_df)\n",
    "\n",
    "def get_user_eventstream(eventstream, userid):\n",
    "    user_stream = la.select_user(eventstream, userid)\n",
    "    user_stream_df = user_stream.df.sort_values([\"contentsid\", \"eventtime\"])\n",
    "    user_stream_df = user_stream_df[[\"contentsid\", \"operationname\",\"eventtime\"]]\n",
    "    df = user_stream_df.replace(Operation_dict)\n",
    "    df.index = np.arange(0, len(df))  \n",
    "    return   df\n",
    "\n",
    "def get_oneuser_sentences(user_eventstream_df,word_max_len=15)->str:\n",
    "        \"\"\"\n",
    "        user_eventstreamからsentenceを生成する。\n",
    "        改行条件:\n",
    "        1. contentsidが変化した場合\n",
    "        2. interval_wordがlの場合(5分以上オペレーション間の時間が空いた場合)\n",
    "        単語の分割条件:\n",
    "        1. 単語の長さがword_max_len以上になった場合\n",
    "        2. 単語の先頭から1分以上たった場合\n",
    "        params:\n",
    "        user_event_stream_df : ユーザーごとのイベントストリーム\n",
    "        word_max_len : 単語の最大長さ。これ以上になったら、強制的に分割する\n",
    "    \n",
    "        return:\n",
    "        sentences : user_event_streamをsentencesに変換したもの\n",
    "        \"\"\"\n",
    "    \n",
    "        sentences = \"\" #返り値用の変数\n",
    "        word = \"\" #単語作成用の一時変数\n",
    "        user_eventstream_df = user_eventstream_df.reset_index(drop=True) #indexを0始まりに直す\n",
    "        for index, data in user_eventstream_df.iterrows():\n",
    "            current = dt.datetime.strptime((data[\"eventtime\"]), '%Y-%m-%d %H:%M:%S') #word_timeによる条件の作動のための変数 \n",
    "            current_contents_id = data[\"contentsid\"] #現在のコンテンツID\n",
    "\n",
    "            if index == 0:#初期化処理\n",
    "                previous = current \n",
    "                previous_contents_id = current_contents_id #最初はpreviousも同じにする\n",
    "                start = current\n",
    "                end = start + dt.timedelta(minutes=1)\n",
    "\n",
    "            if (previous_contents_id != current_contents_id):#コンテンツがIDが変わった場合\n",
    "                sentences += word + \"\\n\"\n",
    "                word = \"\"\n",
    "                current = dt.datetime.strptime(data[\"eventtime\"], '%Y-%m-%d %H:%M:%S')\n",
    "                start = current\n",
    "                end = start + dt.timedelta(minutes=1)\n",
    "            else:   \n",
    "                #前のログからの時間を計算して、該当文字を追加\n",
    "                current_contents_id = data[\"contentsid\"]\n",
    "                current = dt.datetime.strptime(data[\"eventtime\"], '%Y-%m-%d %H:%M:%S')\n",
    "                interval_sec = current - previous\n",
    "                interval_sec = interval_sec.seconds\n",
    "                interval_word = interval_check(interval_sec)\n",
    "                word += interval_word\n",
    "\n",
    "                if interval_word == \"l\":#長時間ログが空いた場合の処理\n",
    "                    sentences += word + \"\\n\"\n",
    "                    word = \"\"\n",
    "                    current = dt.datetime.strptime(data[\"eventtime\"], '%Y-%m-%d %H:%M:%S')\n",
    "                    start = current\n",
    "                    end = start + dt.timedelta(minutes=1)\n",
    "                \n",
    "                if len(word) >= word_max_len - 1: #最大単語数による分割のチェック\n",
    "                    sentences += word + \"_ \"\n",
    "                    word = \"\"\n",
    "                    current = dt.datetime.strptime(data[\"eventtime\"], '%Y-%m-%d %H:%M:%S')\n",
    "                    start = current\n",
    "                    end = start + dt.timedelta(minutes=1)\n",
    "\n",
    "                \n",
    "                if current > end: # wordの先頭から一定時間以上のログ(新しい単語の先頭になるログ)\n",
    "                    sentences += word + \" \"\n",
    "                    word = \"\"\n",
    "                    current = dt.datetime.strptime(data[\"eventtime\"], '%Y-%m-%d %H:%M:%S')\n",
    "                    start = current\n",
    "                    end = start + dt.timedelta(minutes=1)\n",
    "\n",
    "            #wordへの単語の追加\n",
    "            word += data[\"operationname\"]\n",
    "\n",
    "            #各種変数の更新処理\n",
    "            previous = current\n",
    "            previous_contents_id = current_contents_id\n",
    "\n",
    "        else: #最後はwordを追加\n",
    "            sentences += word\n",
    "\n",
    "        if sentences != \"\":\n",
    "            sentences += \"\\n\"\n",
    "        \n",
    "        return sentences \n",
    "    \n",
    "def interval_check(interval_sec:int):\n",
    "    if interval_sec <= 1:\n",
    "      interval_word = ''\n",
    "    elif interval_sec > 1 and interval_sec <= 10:\n",
    "      interval_word = 's'\n",
    "    elif interval_sec > 10 and interval_sec <= 300:\n",
    "      interval_word = 'm'    \n",
    "    else:\n",
    "      interval_word = 'l'\n",
    "    return interval_word\n",
    "\n",
    "  # ファイル書き出し\n",
    "def write_sentences(file_path, actions,usersid,train_flg=0):\n",
    "    i = 0\n",
    "    f = open(file_path, 'w')\n",
    "    for action in actions:\n",
    "      if action != None:\n",
    "        f.write(action)\n",
    "        if train_flg ==0:\n",
    "            f.write('****{}****\\n'.format(usersid[i]))\n",
    "        i+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EventStream to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Actions_train(course_id):\n",
    "    print(course_id)\n",
    "    actions_file = actions_txt_path + r\"\\actions_{}.txt\".format(course_id)\n",
    "    # 指定のコースのEventStream を取得\n",
    "    course_info = la.CourseInformation(files_dir=Edudata, course_id=course_id)\n",
    "    eventstream = course_info.load_eventstream()\n",
    "    #print(eventstream)\n",
    "    # get students' user id in selected course\n",
    "    usersid = course_info.user_id()\n",
    "    #print(usersid)\n",
    "    # get actions from student activity in selected course\n",
    "    actions=[get_learninglog_sentences(eventstream,userid) for userid in usersid]\n",
    "    # save file\n",
    "    write_sentences(actions_file, actions,usersid,train_flg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course in courses:\n",
    "    get_Actions_train(course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make ALL-2020 actions textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(dir, pattern):\n",
    "    matched_files = []\n",
    "    regex = re.compile(pattern)\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if regex.match(file):\n",
    "                matched_files.append(os.path.join(root, file))\n",
    "    return matched_files\n",
    "\n",
    "def concat_files(files,year):\n",
    "    with open(actions_txt_path + r\"\\actions_ALL-{}.txt\".format(year), \"w\") as new_file:\n",
    "        for name in files:\n",
    "            with open(name) as f:\n",
    "                for line in f:\n",
    "                    new_file.write(line)           \n",
    "                new_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2020\"\n",
    "pattern = r'actions_[A-Z]-{}.txt'.format(year)\n",
    "matched_fiels = find_files(actions_txt_path, pattern)\n",
    "concat_files(matched_fiels,year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actions to making vector (split by each user) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Actions_Students(course_id):\n",
    "    actions_file = actions_txt_path + r\"\\actions_{}_perStudents.txt\".format(course_id)\n",
    "    # 指定のコースのEventStream を取得\n",
    "    course_info = la.CourseInformation(files_dir=Edudata, course_id=course_id)\n",
    "    eventstream = course_info.load_eventstream()\n",
    "    # get students' user id in selected course\n",
    "    usersid = eventstream.user_id()\n",
    "    # get actions from student activity in selected course\n",
    "    actions=[get_learninglog_sentences(eventstream, userid) for userid in usersid]\n",
    "    # save file\n",
    "    write_sentences(actions_file, actions,usersid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course in courses:\n",
    "    get_Actions_Students(course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### histgram of the number of each student actions in A-2022 and D-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"userid\", \"num_units\", \"num_actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course_id in [\"A-2022\", \"D-2022\"]:\n",
    "    data_list = []\n",
    "    actions_file = actions_txt_path + r\"\\actions_{}_perStudents.txt\".format(course_id)\n",
    "    with open(actions_file,\"r\") as f:\n",
    "        num_word = 0\n",
    "        num_sentences = 0\n",
    "        for line in f:\n",
    "            if line.startswith(\"****\"):\n",
    "                userid = line.strip()\n",
    "                userid = userid.strip(\"****\")\n",
    "                data_list.append([userid,num_word,num_sentences])\n",
    "                num_word = 0\n",
    "                num_sentences = 0\n",
    "            else:\n",
    "                num_word += len(line.strip().split(' '))\n",
    "                num_sentences +=1\n",
    "    df = pd.DataFrame(data_list,columns=columns)\n",
    "    df = df.set_index(\"userid\")\n",
    "    df.to_csv(actions_txt_path + r'\\{}_words_sentences_count.csv'.format(course_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.read_csv(actions_txt_path + r'\\{}_words_sentences_count.csv'.format(\"A-2022\"))\n",
    "df_d = pd.read_csv(actions_txt_path + r'\\{}_words_sentences_count.csv'.format(\"D-2022\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = df_a[\"num_actions\"]\n",
    "data2 = df_d[\"num_actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.histogram_bin_edges(np.concatenate((data1, data2)), bins=30)\n",
    "\n",
    "# draw histgram\n",
    "n1, _ = np.histogram(data1, bins=bins)\n",
    "n2, _ = np.histogram(data2, bins=bins)\n",
    "\n",
    "plt.hist(bins[:-1],bins, weights=n1, alpha=0.5, label=\"A-2022\")\n",
    "plt.hist(bins[:-1], bins, weights=n2, alpha=0.5, label=\"D-2022\")\n",
    "plt.legend()\n",
    "max_height = max(max(n1), max(n2))\n",
    "plt.ylim(0, max_height)\n",
    "plt.xlabel('The number of actions')\n",
    "plt.ylabel('The number of users')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
