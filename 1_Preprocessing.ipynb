{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import OpenLA as la\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_txt_path = r\".\\data\\actions_txt\"\n",
    "Edudata = r'.\\data\\EduData_20221028'\n",
    "courses = [\"A-2020\",\"D-2020\",\"A-2021\",\"D-2021\",\"A-2022\",\"D-2022\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Operation_dict = {\n",
    "        'NEXT':'N',\n",
    "        'PREV':'P',\n",
    "        'ADD MARKER':'A',\n",
    "\n",
    "        'OPEN':'O',             # 'E'\n",
    "        'CLOSE':'C',            # 'E'\n",
    "        'PAGE_JUMP':'J',        # 'E'\n",
    "        'GETIT':'G',            # 'E'\n",
    "\n",
    "        'DELETE MARKER':'E',\n",
    "        'BOOKMARK_JUMP':'E',\n",
    "        'ADD BOOKMARK':'E',\n",
    "        'NOTGETIT':'E',\n",
    "        'ADD MEMO':'E',\n",
    "        'MEMO_TEXT_CHANGE_HISTORY':'E',\n",
    "\n",
    "        'DELETE BOOKMARK':'E',\n",
    "        'CHANGE MEMO':'E',\n",
    "        'SEARCH_JUMP':'E',\n",
    "        'REGIST CONTENTS':'E',\n",
    "        'DELETE_MEMO':'E',\n",
    "        'SEARCH':'E',\n",
    "        'OPEN_RECOMMENDATION':'E',\n",
    "        'CLICK_RECOMMENDATION':'E',\n",
    "        'TIMER_PAUSE':'E',\n",
    "        'TIMER_STOP':'E',\n",
    "        'ADD_HW_MEMO':'E',\n",
    "\n",
    "        'CLOSE_RECOMMENDATION':'E',\n",
    "        'CLEAR_HW_MEMO':'E',\n",
    "        'LINK_CLICK':'E',\n",
    "        'UNDO_HW_MEMO':'E',\n",
    "        'ADD_RECOMMENDATION' : 'E',\n",
    "        'REDO_HW_MEMO' : 'E',\n",
    "        'DELETE_RECOMMENDATION' : 'E',\n",
    "        'MEMO_JUMP' : 'E'\n",
    "        }\n",
    "\n",
    "\n",
    "def get_learninglog_sentences(eventstream, userid):\n",
    "    user_eventstream_df = get_user_eventstream(eventstream,userid)\n",
    "    return get_oneuser_sentences(user_eventstream_df)\n",
    "\n",
    "def get_user_eventstream(eventstream, userid):\n",
    "    user_stream = la.select_user(eventstream, userid)\n",
    "    user_stream_df = user_stream.df.sort_values([\"contentsid\", \"eventtime\"])\n",
    "    user_stream_df = user_stream_df[[\"contentsid\", \"operationname\",\"eventtime\"]]\n",
    "    df = user_stream_df.replace(Operation_dict)\n",
    "    df.index = np.arange(0, len(df))  \n",
    "    return   df\n",
    "\n",
    "def get_oneuser_sentences(user_eventstream_df):\n",
    "  # sentences : output\n",
    "  # word     : component of sentence\n",
    "  # current  : event time of current operation\n",
    "  # previous : event time of previous operation\n",
    "  # cc       : lecuture materials which the current operation took place\n",
    "  # pc       : lecuture materials which the previous operation took place\n",
    "  # start    : Time stamp at the beginning of the word\n",
    "  # end      : time limit of currentword \n",
    "    sentences = \"\"\n",
    "    word = \"\"\n",
    "    current = dt.datetime.now()\n",
    "    end = dt.datetime.now()\n",
    "    cc  = \"\"\n",
    "    word_max_len = 15\n",
    "    for index, data in user_eventstream_df.iterrows():\n",
    "      if index == 0:                                                             \n",
    "          current = dt.datetime.strptime((data[\"eventtime\"]), '%Y-%m-%d %H:%M:%S')\n",
    "          cc = data[\"contentsid\"]\n",
    "      else:\n",
    "          pc = cc\n",
    "          cc = data[\"contentsid\"]\n",
    "          if pc != cc:\n",
    "            sentences += (word + '\\n')\n",
    "            word = data['operationname']\n",
    "            current = dt.datetime.strptime(data[\"eventtime\"], '%Y-%m-%d %H:%M:%S')\n",
    "            start = current\n",
    "            end = start + dt.timedelta(minutes=1)\n",
    "            continue\n",
    "          else:\n",
    "            previous = current\n",
    "            current = dt.datetime.strptime(data[\"eventtime\"], '%Y-%m-%d %H:%M:%S')\n",
    "            interval_sec = current - previous\n",
    "            interval_sec = interval_sec.seconds\n",
    "            interval_word = interval_check(interval_sec)\n",
    "            word += interval_word\n",
    "            if interval_word == 'l' :\n",
    "              sentences += (word + '\\n')\n",
    "              word = data['operationname']\n",
    "              start = current\n",
    "              end = start + dt.timedelta(minutes=1)\n",
    "              if index != len(user_eventstream_df)-1:\n",
    "                continue\n",
    "            elif  len(word) >= word_max_len - 1:\n",
    "              sentences += (word + '_ ')\n",
    "              word = data['operationname']\n",
    "              start = current\n",
    "              end = start + dt.timedelta(minutes=1)\n",
    "              continue\n",
    "      if index == len(user_eventstream_df)-1:\n",
    "        if index == 0:\n",
    "          sentences = data['operationname'] \n",
    "        elif current <= end:\n",
    "          word += data['operationname'] \n",
    "          sentences += word\n",
    "        else:\n",
    "          sentences += (word + ' ')\n",
    "          sentences += data['operationname']\n",
    "      elif index == 0:\n",
    "          start = current\n",
    "          end = start + dt.timedelta(minutes=1)\n",
    "          word = data['operationname']\n",
    "      elif current <= end:\n",
    "          word += data[\"operationname\"]\n",
    "      else :\n",
    "        sentences += (word + ' ')\n",
    "        word = data[\"operationname\"]\n",
    "        start = current\n",
    "        end = start + dt.timedelta(minutes=1)   \n",
    "    if sentences != \"\":\n",
    "        return (sentences + '\\n') \n",
    "    \n",
    "def interval_check(interval_sec:int):\n",
    "    if interval_sec <= 1:\n",
    "      interval_word = ''\n",
    "    elif interval_sec > 1 and interval_sec <= 10:\n",
    "      interval_word = 's'\n",
    "    elif interval_sec > 10 and interval_sec <= 300:\n",
    "      interval_word = 'm'    \n",
    "    else:\n",
    "      interval_word = 'l'\n",
    "    return interval_word\n",
    "\n",
    "  # ファイル書き出し\n",
    "def write_sentences(file_path, actions,usersid,train_flg=0):\n",
    "    i = 0\n",
    "    f = open(file_path, 'w')\n",
    "    for action in actions:\n",
    "      if action != None:\n",
    "        f.write(action)\n",
    "        if train_flg ==0:\n",
    "            f.write('****{}****\\n'.format(usersid[i]))\n",
    "        i+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EventStream to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Actions_train(course_id):\n",
    "    print(course_id)\n",
    "    actions_file = actions_txt_path + r\"\\actions_{}.txt\".format(course_id)\n",
    "    # 指定のコースのEventStream を取得\n",
    "    course_info = la.CourseInformation(files_dir=Edudata, course_id=course_id)\n",
    "    eventstream = course_info.load_eventstream()\n",
    "    #print(eventstream)\n",
    "    # get students' user id in selected course\n",
    "    usersid = course_info.user_id()\n",
    "    #print(usersid)\n",
    "    # get actions from student activity in selected course\n",
    "    actions=[get_learninglog_sentences(eventstream,userid) for userid in usersid]\n",
    "    # save file\n",
    "    write_sentences(actions_file, actions,usersid,train_flg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course in courses:\n",
    "    get_Actions_train(course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make ALL-2020 actions textfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(dir, pattern):\n",
    "    matched_files = []\n",
    "    regex = re.compile(pattern)\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if regex.match(file):\n",
    "                matched_files.append(os.path.join(root, file))\n",
    "    return matched_files\n",
    "\n",
    "def concat_files(files,year):\n",
    "    with open(actions_txt_path + r\"\\actions_ALL-{}.txt\".format(year), \"w\") as new_file:\n",
    "        for name in files:\n",
    "            with open(name) as f:\n",
    "                for line in f:\n",
    "                    new_file.write(line)           \n",
    "                new_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2020\"\n",
    "pattern = r'actions_[A-Z]-{}.txt'.format(year)\n",
    "matched_fiels = find_files(actions_txt_path, pattern)\n",
    "concat_files(matched_fiels,year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actions to making vector (split by each user) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Actions_Students(course_id):\n",
    "    actions_file = actions_txt_path + r\"\\actions_{}_perStudents.txt\".format(course_id)\n",
    "    # 指定のコースのEventStream を取得\n",
    "    course_info = la.CourseInformation(files_dir=Edudata, course_id=course_id)\n",
    "    eventstream = course_info.load_eventstream()\n",
    "    # get students' user id in selected course\n",
    "    usersid = eventstream.user_id()\n",
    "    # get actions from student activity in selected course\n",
    "    actions=[get_learninglog_sentences(eventstream, userid) for userid in usersid]\n",
    "    # save file\n",
    "    write_sentences(actions_file, actions,usersid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course in courses:\n",
    "    get_Actions_Students(course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histgram of the number of each student actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"userid\", \"num_units\", \"num_actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for course_id in [\"A-2022\", \"D-2022\"]:\n",
    "    data_list = []\n",
    "    actions_file = actions_txt_path + r\"\\actions_{}_perStudents.txt\".format(course_id)\n",
    "    with open(actions_file,\"r\") as f:\n",
    "        num_word = 0\n",
    "        num_sentences = 0\n",
    "        for line in f:\n",
    "            if line.startswith(\"****\"):\n",
    "                userid = line.strip()\n",
    "                userid = userid.strip(\"****\")\n",
    "                data_list.append([userid,num_word,num_sentences])\n",
    "                num_word = 0\n",
    "                num_sentences = 0\n",
    "            else:\n",
    "                num_word += len(line.strip().split(' '))\n",
    "                num_sentences +=1\n",
    "    df = pd.DataFrame(data_list,columns=columns)\n",
    "    df = df.set_index(\"userid\")\n",
    "    df.to_csv(actions_txt_path + r'\\{}_words_sentences_count.csv'.format(course_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = pd.read_csv(actions_txt_path + r'\\{}_words_sentences_count.csv'.format(\"A-2022\"))\n",
    "df_d = pd.read_csv(actions_txt_path + r'\\{}_words_sentences_count.csv'.format(\"D-2022\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = df_a[\"num_actions\"]\n",
    "data2 = df_d[\"num_actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.histogram_bin_edges(np.concatenate((data1, data2)), bins=30)\n",
    "\n",
    "# draw histgram\n",
    "n1, _ = np.histogram(data1, bins=bins)\n",
    "n2, _ = np.histogram(data2, bins=bins)\n",
    "\n",
    "plt.hist(bins[:-1], bins, weights=n2, alpha=0.7)\n",
    "plt.legend()\n",
    "max_height = max(max(n1), max(n2))\n",
    "plt.ylim(0, max_height)\n",
    "plt.xlabel('The number of actions')\n",
    "plt.ylabel('The number of users')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
